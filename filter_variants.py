import os
import sys
import argparse
import numpy as np
import pandas as pd

def parse_args():
    ''' Parse and check command line arguments.'''

    # argument parsing
    parser = argparse.ArgumentParser(description = "Generate UK Biobank variant filter lists")
    parser.add_argument("mfi_file", help = "Input UK Biobank mfi file")
    parser.add_argument("hwe_file",
            help = "Input HWE test file, generated by plink2 --hardy")
    parser.add_argument("miss_file",
            help = "Input marker missingness file, generated by plink2 --missing")
    parser.add_argument("--freq",
            help = "Input marker frequency file, generated by plink2 --freq. If supplied, will use MAF in this file for MAF filtering. Otherwise, will use MAF in the MFI file.")
    parser.add_argument("-o", "--output", default = "excl_variants.txt",
            help = "Output file for variants exclude.")
    parser.add_argument("--info", type = float, default = 0.5,
            help = "Exclude variants with INFO score less than this number. Default: 0.5.")
    parser.add_argument("--hwe", type = float, default = 1e-10,
            help = "Exclude variants with HWE p-value less than this number. Default: 1e-10.")
    parser.add_argument("--maf", type = float, default = 0,
            help = "Exclude variants with MAF less than this number.  Default: 0.")
    parser.add_argument("--missingness", type = float, default = 0.05,
            help = "Exclude variants that are missing above this threshold. Default: 0.05")

    args = parser.parse_args()
    '''
    args = parser.parse_args(["/n/holylfs/LABS/xlin_lab_genetics/UKB/gwas_data/imputed/ukb_mfi_chr22_v3.txt",
        "/n/holylfs/LABS/xlin_lab_genetics/UKB/gwas_data/imputed/ukb_imp_chr22_v3_stats.hardy",
        "/n/holylfs/LABS/xlin_lab_genetics/UKB/gwas_data/imputed/ukb_imp_chr22_v3_stats.vmiss",
        "--freq",
        "/n/holylfs/LABS/xlin_lab_genetics/UKB/gwas_data/imputed/ukb_imp_chr22_v3_stats.afreq",
        "--maf", "1e-4"])
    '''

    # input checking
    # check files
    def check_file(fname, ftype):
        ''' Return true if fname exists or is None '''
        if fname is None:
            return True
        file_exists = os.path.isfile(fname)
        if not file_exists:
            print("{ftype} file {fname} is not a file or does not "
                    "exist.".format(ftype = ftype, fname = fname))
        return(file_exists)

    files_tocheck = [args.mfi_file, args.hwe_file, args.freq,
            args.miss_file]
    ftypes = ["MFI", "HWE", "Frequency", "Missingness"]
    assert(len(files_tocheck) == len(ftypes))
    file_checks = map(check_file, files_tocheck, ftypes)
    if not all(file_checks):
        sys.exit("At least one required input file does not exist.")

    # check QC parameters
    if (args.missingness < 0 or args.missingness > 1):
        sys.exit("Missingness threshold must be between 0 and 1.")
    if (args.hwe <= 0 or args.hwe > 1):
        sys.exit("HWE threshold must be between 0 and 1.")
    if (args.info < 0):
        sys.exit("INFO threshold must be >= 0.")
    if (args.maf < 0 or args.maf > 0.5):
        sys.exit("MAF threshold must be between 0 and 0.5.")

    return(args)

def main():
    args = parse_args()

    print("SNP filtering arguments used:")
    for (arg_name, arg_value) in vars(args).items():
        print("%s: %s" %(arg_name, arg_value))

    # ------------------------------------------------------------------
    # read in all the files
    # ------------------------------------------------------------------
    mfi_fields = ["alternate_id", "rsid", "position", "allele1",
            "allele2", "maf", "minor_allele", "info"]
    mfi_df = pd.read_csv(args.mfi_file, sep = "\t",
            names = mfi_fields, header = None)
    hwe_df = pd.read_csv(args.hwe_file, sep = "\t")
    vmiss_df = pd.read_csv(args.miss_file, sep = "\t")

    assert(mfi_df.shape[0] == hwe_df.shape[0])
    assert(vmiss_df.shape[0] == hwe_df.shape[0])
    assert(np.all(mfi_df.rsid == hwe_df.ID))
    assert(np.all(vmiss_df.ID == hwe_df.ID))
    num_snps = mfi_df.shape[0]

    if args.freq is None:
        df_to_combine = [mfi_df, hwe_df, vmiss_df]
    else:
        freq_df = pd.read_csv(args.freq, sep = "\t")
        # convert allele frequency to minor allele frequency
        freq_df['plink_maf'] = np.where(freq_df['ALT_FREQS'] < 0.5,
                freq_df['ALT_FREQS'], 1 - freq_df['ALT_FREQS'])
        assert(freq_df.shape[0] == hwe_df.shape[0])
        assert(np.all(freq_df.ID == hwe_df.ID))
        df_to_combine = [mfi_df, hwe_df, vmiss_df, freq_df]

    df_all = pd.concat(df_to_combine, axis = 1)
    assert(df_all.shape[0] == num_snps)

    # ------------------------------------------------------------------
    # filtering
    # ------------------------------------------------------------------
    mfi_excl = np.array(df_all['info'] < args.info)
    hwe_excl = np.array(df_all['P'] < args.hwe)
    vmiss_excl = np.array(df_all['F_MISS'] > args.missingness)

    if args.maf > 0:
        if args.freq is None:
            maf_excl = np.array(df_all['maf'] < args.maf)
        else:
            maf_excl = np.array(df_all['plink_maf'] < args.maf)
        excl_arr = np.column_stack((mfi_excl, hwe_excl, vmiss_excl, maf_excl))
        assert(excl_arr.shape == (num_snps, 4))
    else:
        excl_arr = np.column_stack((mfi_excl, hwe_excl, vmiss_excl))
        assert(excl_arr.shape == (num_snps, 3))

    excl_idx = np.any(excl_arr, axis = 1)
    excl_snp_df = df_all.loc[excl_idx, 'rsid']
    num_removed = excl_snp_df.shape[0]

    print("Summary of SNP filtering:")
    print("%d / %d (%0.1f%%) SNPs removed."
            %(num_removed, num_snps, 100 * num_removed / num_snps))
    print("Writing excluded SNPs to file %s..." %args.output)
    excl_snp_df.to_csv(args.output, index = False, header = False,
            sep = "\t")

if __name__ == "__main__":
    main()
